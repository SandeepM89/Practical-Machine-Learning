---
title: "Practical Machine Learning Prediction Assignment - Weight Lifting Exercise"
author: "Sandeep Mahapatra"
date: "August 8, 2016"
output: html_document
---

```` {r}

#Summary

#Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways.

#The goal of this project is to predict the manner in which they did the exercises. This is the "classe" variable in the training set which classifies the correct and incorrect into 5 categories namely A, B, C, D and E. This report describes how I have built the model, used cross validation, expected out of sample error calculation and the related choices made. 

#The above mentioned procedure has been used successfully to predict 20 different test cases by applying the machine learning algorithm built herein to solve the Prediction Quiz on the Coursera Website for this Prediction Assignment.

#Data Description

#The data was downloaded and saved to a local directory

setwd("C:/Users/D1/Desktop/StratLytics Projects/Coursera/Practical Machine Learning/Week 4")
training <- read.csv("pml-training.csv", na.strings = c("NA", "#DIV/0!", ""))
testing <- read.csv("pml-testing.csv",  na.strings = c("NA", "#DIV/0!", ""))

#Cleaning the data

#Removing the first 8 columns from the data as they are not required information for the analysis ahead and removing all the columns with NA values

training <- training[, 8:160]
testing  <- testing[, 8:160]

#Splitting the training dataset in two parts (60:40) for cross validation purposes
#install.packages("caret", dependencies = TRUE)
library(caret)
set.seed(3141592)
inTrain <- createDataPartition(y=training$classe, p=0.60, list=FALSE)
train1  <- training[inTrain,]
train2  <- training[-inTrain,]
# train1 is the training data set (it contains 11776 observations, or about 60% of the entire training data set), and train2 is the testing data set (it contains 7846 observations, or about 40% of the entire training data set)
print(dim(train1))
print(dim(train2))
#removing the near zero variance variables
myDataNZV <- nearZeroVar(train1, saveMetrics=TRUE)
myNZVvars <- names(train1) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
"kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
"max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
"var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
"stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
"kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
"max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
"kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
"skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
"amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
"skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
"max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
"amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
"avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
"stddev_yaw_forearm", "var_yaw_forearm")
train1 <- train1[!myNZVvars]
#dataset creation for loop
training_loop <- train1 
for(i in 1:length(train1)) { 
        if( sum( is.na( train1[, i] ) ) /nrow(train1) >= .6 ) { 
        for(j in 1:length(training_loop)) {
            if( length( grep(names(train1[i]), names(training_loop)[j]) ) ==1)  { 
                training_loop <- training_loop[ , -j] 
            }   
        } 
    }
}
train1 <- training_loop
rm(training_loop)
gc(reset=TRUE)

clean1 <- colnames(train1)
clean2 <- colnames(train1[, -53]) 
train1 <- train1[clean1]
testing <- testing[clean2]
print(dim(train2))
#In order to ensure proper functioning of Decision Trees and especially RandomForest Algorithm with the Test data set, we need to coerce the data into the same type
for (i in 1:length(testing) ) {
        for(j in 1:length(train1)) {
        if( length( grep(names(train1[i]), names(testing)[j]) ) ==1)  {
            class(testing[j]) <- class(train1[i])
        }      
    }      
}

#Using Machine Learning Algorithm to predict decision tree
library(rpart)
FitModel <- rpart(classe ~ ., data=train1, method="class")
library(rpart.plot)
prp(FitModel)
#For Predictions (In-Sample Error)
prediction <- predict(FitModel, train2, type = "class")
library(caret)
DTCM <- confusionMatrix(prediction, train2$classe)
print(DTCM)

#Test for the Out of Sample Error calculation
Cal <- function(values, predicted) {
  sum(predicted != values) / length(values)
}
OutofSample_ErrorRate <- Cal(train2$classe, prediction)
print(OutofSample_ErrorRate)

#Conclusion

#From the above analysis i can conclude that the data provides a reasonable opportunity to present a very accurate model with a prediction of 75% and Out of Sample Error calculated at 25%.

````
